{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 â€“ Logistic Regression task\n",
    "1.1.1 Describe the dataset (e.g., descriptive statistics, missing values, target rate).\n",
    "\n",
    "1.1.2 Describe the feature engineering procedure and the data treatments you followed (if any).\n",
    "\n",
    "1.1.3 Describe the model selection process you applied (e.g., criteria for feature selection, estimation technique of the model parameters).\n",
    "\n",
    "1.1.4 Explain the final model in terms of statistical results and business interpretation of regression coefficients.\n",
    "\n",
    "1.1.5 Present the assumptions of the logistic regression and check if they are fulfilled by your model.\n",
    "\n",
    "1.1.6 Calculate the following performance metrics: Accuracy, Precision, Recall and F1 score both in Testing and Training samples.\n",
    "\n",
    "1.1.7 Create the ROC curve (AUC) and explain the discriminatory power of the model both in Testing and Training samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import calendar\n",
    "import random\n",
    "import time\n",
    "from time import perf_counter, sleep\n",
    "from functools import wraps\n",
    "from typing import Callable, Any\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from scipy.stats import mode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "import optuna\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = pd.read_excel('https://challengerocket.com/files/lions-den-ing-2024/variables_description.xlsx', header=0)\n",
    "train = pd.read_csv('https://files.challengerocket.com/files/lions-den-ing-2024/development_sample.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(train[train['Application_status']=='Rejected'].shape[0]/train.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset is build upon 50 000 records an 36 columns. Test dataset consist of 5000 records. The number of rejected applications (that need to be removed) makes around 26% of the train dataset. For the further clarity of the analysis, let's drop rejected applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train[train['Application_status']=='Approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loans that went into default make only 3% of total applications (in this dataset). Which makes it very unbalanced. Which would be addressed in the further analysis with diffrent methods for diffrent models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process further we need to divide features into categorical and numerical. The division is based on the close analysis of column descriptions and values. Additionaly we catched columns that won't be taken into account by our model. Furthermore Var13 - date of employment is transformed into the number of days between employment and application_date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_features = ['ID','customer_id', 'application_date', 'Application_status', 'Var13', '_r_']\n",
    "\n",
    "target = 'target'\n",
    "\n",
    "categorical_features = ['Var2', 'Var3', 'Var11','Var12', 'Var14','Var18', 'Var19']\n",
    "\n",
    "binary_features = ['Var27', 'Var28']\n",
    "\n",
    "days_from_employment = 'Var13_b'\n",
    "\n",
    "numerical_features = ['Var1', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8', 'Var9', 'Var10', 'Var15', 'Var16', 'Var17', 'Var20', 'Var21', 'Var22', 'Var23', 'Var24', 'Var25', 'Var26', 'Var29', 'Var30', days_from_employment]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get days from employment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.to_datetime(df['application_date'].copy(), format='%d%b%Y %H:%M:%S', errors='coerce')\n",
    "d2 = pd.to_datetime(df['Var13'], format='%d%b%Y', errors='coerce')\n",
    "df[days_from_employment] = (d1 - d2).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descirption of numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of numerical values can be done quickly by pandas built-in function descirbe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the proportion of every group within the given feature. Such table is easy to filter by features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_desc = pd.DataFrame(columns=['value', 'proportion', 'column'])\n",
    "\n",
    "for feature in categorical_features + binary_features:\n",
    "    x = df[feature].value_counts(normalize=True).reset_index().rename(columns={feature:'value'})\n",
    "    x['column'] = feature\n",
    "    categorical_features_desc = pd.concat([categorical_features_desc, x], axis=0, ignore_index=True)\n",
    "\n",
    "categorical_features_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we checked all columns with NaN values. Next, to check if the NaN values may correlate with the target, we performed a chi-square test of independent for every column with missing values. As in some cases NaN value could indicate that given person would be more likely to default or less likely to default ie NaN value could be a signal information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nans = df.isna().sum()\n",
    "columns_w_nans = list_of_nans[list_of_nans>0]\n",
    "\n",
    "# Set up an empty dict for results\n",
    "nan_values_correlation = {}\n",
    "\n",
    "for column in list(columns_w_nans.index):\n",
    "  temp_df = df[[column, 'target']].copy()\n",
    "  # Divde column values into NaN and not-NaN values\n",
    "  temp_df[column + '_na'] = temp_df[column].isna().apply(lambda x: 'none' if x else 'not_none')\n",
    "  # Get a crosstab\n",
    "  crosstab = pd.crosstab(temp_df[column + '_na'], temp_df['target'])\n",
    "  # Get p-value\n",
    "  chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "  nan_values_correlation[column]=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table = pd.concat([columns_w_nans/df.shape[0], pd.Series(nan_values_correlation)\n",
    "], axis=1, keys=['missing_rate', 'p-value']).reset_index(names='column')\n",
    "missing_values_table['data type'] = missing_values_table['column'].apply(lambda x: 'categorical' if x in categorical_features else ('numerical' if x in numerical_features else 'other'))\n",
    "missing_values_table.merge(right = description, left_on ='column', right_on='Column')\n",
    "missing_values_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Describe the feature engineering procedure and the data treatments you followed (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we decided to fill some of the missing values with either 0, a new category \"other\" or with the median. It was dependent on the analysis and interpretation of every single column nature. Furthermore, we decided to drop columns Var10 and Var12 as they didn't pass the 0.05 significance test and had over 75% of data missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backupt = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_with_zero = ['Var8', 'Var25', 'Var26', days_from_employment]\n",
    "add_other_category = ['Var18', 'Var19', 'Var2', 'Var3']\n",
    "\n",
    "df['Var17'].fillna(df['Var17'].median(), inplace=True)\n",
    "\n",
    "for var in fill_with_zero: \n",
    "    df[var].fillna(0, inplace=True)\n",
    "\n",
    "for var in add_other_category: \n",
    "    df[var].fillna('other', inplace=True)\n",
    "\n",
    "columns_to_drop = ['Var10', 'Var12']\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we decided to change categorical features which required one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_dummies = set(categorical_features)-set(columns_to_drop)\n",
    "\n",
    "for feature in need_dummies:\n",
    "  one_hot = pd.get_dummies(df[feature], prefix=feature, drop_first=True).astype(int)\n",
    "  df = df.drop(feature, axis = 1)\n",
    "  df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Describe the model selection process you applied (e.g., criteria for feature selection, estimation technique of the model parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we decided to find highly correlated features (>0.7) using correlation matrix. Next, we decided to perform two analyses: Variable Importance in Projection and Variable Importance Factor. These allowed as to decide which of correlated features can we drop and let us drop other features with low importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlation = list(set(numerical_features+[days_from_employment])-set(columns_to_drop))\n",
    "\n",
    "# Get correlation matrix \n",
    "corr_matrix= df[get_correlation].corr()\n",
    "\n",
    "# Flatten correlation matrix for visibility \n",
    "corr_flattened = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "corr_flattened\n",
    "pairwise_corr = corr_flattened.stack().reset_index()\n",
    "pairwise_corr.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
    "\n",
    "# Get absolute values (for negativly and positively correlated)\n",
    "pairwise_corr['Correlation_abs']=pairwise_corr['Correlation'].abs()\n",
    "\n",
    "correlation_results = pairwise_corr.sort_values(by='Correlation_abs', ascending=False)\n",
    "correlation_results[correlation_results['Correlation_abs'] > 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIP \n",
    "\n",
    "VIP - Variable Importance in Projection is a method that allows to assess the importance of each variable in the PLS model. It is calculated as a weighted sum of the squared correlations between the predictors and the PLS components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and label \n",
    "X_vip = df[get_correlation]\n",
    "Y_vip = df['target']\n",
    "\n",
    "# Create and fit PLS Regression model \n",
    "pls = PLSRegression(n_components=2)\n",
    "pls.fit(X_vip,Y_vip)\n",
    "\n",
    "# Extract model parameters\n",
    "weights = pls.x_weights_\n",
    "variance = pls.x_scores_.var(axis=0)\n",
    "total_variance = variance.sum()\n",
    "n_components = pls.n_components\n",
    "n_predictors = X_vip.shape[1]\n",
    "\n",
    "# Calculare VIP scores\n",
    "vip_scores = np.sqrt(n_predictors * (weights**2 * variance / total_variance).sum(axis=1) / n_components)\n",
    "\n",
    "# Get column names\n",
    "variable_names = X_vip.columns  \n",
    "vip_dict = dict(zip(variable_names, vip_scores))\n",
    "\n",
    "vip_points = pd.Series(vip_dict, name='vip').sort_values(ascending=False).reset_index()\n",
    "vip_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF\n",
    "\n",
    "VIF - Variance Inflation Factor is a measure of multicollinearity among the independent variables within a multiple regression. It is calculated by taking the the ratio of the variance of all a given model's betas divide by the variane of a single beta if it were fit alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data = pd.DataFrame({\n",
    "    'Feature': X_vip.columns,\n",
    "    'VIF': [variance_inflation_factor(X_vip.values, i) for i in range(X_vip.shape[1])]\n",
    "})\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join both metrics \n",
    "vip_points = vip_points.merge(right=vif_data, left_on='index', right_on='Feature').drop(columns=['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    pd.merge(\n",
    "        correlation_results[correlation_results['Correlation_abs']>0.7], vip_points, left_on='Feature_1', right_on='index'),\n",
    "         vip_points, left_on='Feature_2', right_on='index').drop(columns=['index_x', 'index_y'])\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the table above we decided to drop features: Var21, Var22, Var23, Var16, Var4 and Var30 (those variables are number of request during last 6, 9 and 12 month respectively) Var4 is amount of credit while var 16 is number of dependences of main applicant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_to_drop = ['Var22', 'Var23', 'Var21', 'Var16', 'Var4', 'Var30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = other_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_points.sort_values(by='vip', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data and creating Training, Test and Validation sets. We won't use provided test sample as in real life scenario this data set wouldn't have correct predictions. It will be used in the last section of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop not feature columns \n",
    "df.drop(columns=not_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print(\"Training shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we decided to standarize the data to obtain better results and help optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standarized data \n",
    "scaler = StandardScaler()\n",
    "X_train_logit = scaler.fit_transform(X_train)\n",
    "X_val_logit = scaler.transform(X_val)\n",
    "X_test_logit = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Logit = sm.Logit(y_train, X_train_logit).fit(disp=0, maxiter=10000)  # if fail restart kernel as in some python versions there is a bug\n",
    "model_Logit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "description"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "models_x_to_var = {}\n",
    "models_x = list(model_Logit.params.index)\n",
    "for x in models_x:\n",
    "    num = x[1:len(x)]\n",
    "    models_x_to_var[x] = X.columns[int(num) - 1]\n",
    "\n",
    "models_x_to_label = pd.Series(models_x_to_var).reset_index(name='column')\n",
    "models_x_to_label['column_splitted'] = models_x_to_label['column'].str.split('_').str[0]\n",
    "\n",
    "models_x_to_label.merge(right=description, right_on='Column', left_on='column_splitted')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a standardized logistic regression model, where predictor variables have been scaled to have a mean of 0 and a standard deviation of 1, the coefficient of a variable represents the expected change in the log odds of the outcome for a one standard deviation increase in that predictor variable, holding all other variables constant. Therefore, for variable X8 (Spending estimation), an increase of 1 standard deviation in its value is associated with a decrease in the log odds of the target variable by approximately 0.327. This interpretation is based on the assumption that all other variables in the model are held constant (ceteris paribus). The mean and SD of Spending Estimation before standardization are given as 5486.813655 and 4737.913015, respectively. Which means that when the Spending Estimation (X8) variable increases by one of its own standard deviations (4737.913015) on its original scale, which corresponds to a one-unit increase on its standardized scale (since it has been transformed to have a standard deviation of 1), the log odds of the target variable decreases by approximately 0.327, holding all other variables constant. Interpretation for other variables can be done in a similar way.\n",
    "\n",
    "For positive coefficients, the log odds of the target increase as the predictor increases. For negative coefficients, the log odds of the target decrease as the predictor increases. In (are importance levels XXX p-value<0.01, XX p-value<0.05, X p-value<0.1), we have provided only important variables in ().\n",
    "For positive coefficients we have variables: X3, X4, X8(Spending estimation)(XXX), X9, X13, X14(Arrear in last 12 months) (XXX), X15(application data of employment data of main applicant)(XXX), X16, X17, X24(distribution channel broker)(XXX), X25, X26, X30, X31, X33, X34\n",
    "For negative coefficients we have variables: X1(Number of applicants)(XXX) , X2, X5, X6(Income of main applicant)(XXX), X7, X10, X11, X12, X18, X19(Martial status of Divorced)(X), X20(Martial status of Widowed)(X), X21(loan purpose House Renovation)(X), X22(loan purpose Short cash)(XX), X23(loan purpose Other)(X), X27, X28, X29, X35, X37\n",
    "\n",
    "Higher spending estimations increase impact on the probability of default, which is consistent with the economic intuition. The same goes for the application data of employment data of main applicant as longer experience in the job is associated with lower probability of default. Arrear in last 12 months has negative impact on the probability of default as well. Distribution channel broker has negative impact on the probability of default.\n",
    "\n",
    "In economic terms banks should avoid people with high spendings and loans with car loan purpose. Same interpretations can be done for other variables.\n",
    "\n",
    "3 variables related to loan purpose have negative impact on the probability of default in comparison to basic loan purpose which is car loan. This results would mean that people with car loan as a purpose are more likely to default than people with other loan purposes. For marital status, divorced and widowed people are less likely to default than single people. Additionally more applicants are less likely to default as well as higher income of main applicant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    # Compute metrics\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Display metrics\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "\n",
    "    # Confusion Matrix calculation and display\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Display Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGIT MODEL PREDICTIONS\n",
    "\n",
    "In this section we display the results of predictions for three previously constructed datasets: train, validation and test. Predictions that are consistent between sets used for train, validation and test means that our model have not overfitted.\n",
    "\n",
    "Apart from accuracy and other required metrics we also decided to use balanced accuracy due to the inequalities between 1 and 0 targets.\n",
    "\n",
    "As the output of the logistic regression is a number between 0 and 1, we calculated the optimal cutoff based on G-means (validation set) as we have been not provided with average cost of wrong decision neither with profit of correct one. G-means cut off is a metric that balances sensitivity and specificity, making it particularly useful when dealing with imbalanced datasets or when the cost of false positives and false negatives are unknown or considered to be equal. G-means is effective because it finds a threshold that maintains a balance between true positive rate (TPR) and true negative rate (TNR), thus optimizing the model's performance across different classes.\n",
    "\n",
    "Moreover, to visualize the comparison between the predicted and real target values we used heatmaps. \n",
    "\n",
    "Worth to point out that is not shown in the code (due to the length of the output and space) we have also test logistic model with K-Fold cross validation and the results were consistent with results obtained from the train and validation sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_prob = model_Logit.predict(X_val_logit)\n",
    "\n",
    "# Calculate ROC curve from validation set\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_pred_prob)\n",
    "gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "# Locate the index of the largest G-mean\n",
    "ix = np.argmax(gmeans)\n",
    "\n",
    "# The optimal cutoff is the threshold with the largest G-mean\n",
    "optimal_cutoff_Logit = thresholds[ix]\n",
    "print('Optimal Cut-off:', optimal_cutoff_Logit)\n",
    "\n",
    "# Convert probabilities to binary outcome using the optimal cutoff\n",
    "y_train_pred_optimal = [1 if prob > optimal_cutoff_Logit else 0 for prob in model_Logit.predict(X_train_logit)]\n",
    "y_val_pred_optimal = [1 if prob > optimal_cutoff_Logit else 0 for prob in y_val_pred_prob]\n",
    "y_test_pred_optimal = [1 if prob > optimal_cutoff_Logit else 0 for prob in model_Logit.predict(X_test_logit)]\n",
    "\n",
    "# Evaluate the model using the simple evaluation function\n",
    "print(\"Train set Logit:\")\n",
    "evaluate_predictions(y_train, y_train_pred_optimal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Validation set Logit:\")\n",
    "evaluate_predictions(y_val, y_val_pred_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set \n",
    "print(\"Test set Logit:\")\n",
    "evaluate_predictions(y_test, y_test_pred_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# function to plot ROC curve\n",
    "def plot_roc_curve(fpr, tpr, title='ROC Curve'):\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "y_train_pred_prob = model_Logit.predict(X_train_logit)\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train.astype(int).values, y_train_pred_prob)\n",
    "plot_roc_curve(fpr_train, tpr_train, 'Training Set ROC Curve Logit')\n",
    "\n",
    "y_val_pred_prob = model_Logit.predict(X_val_logit)\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val.astype(int).values, y_val_pred_prob)\n",
    "plot_roc_curve(fpr_val, tpr_val, 'Validation Set ROC Curve Logit')\n",
    "\n",
    "y_test_pred_prob = model_Logit.predict(X_test_logit)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test.astype(int).values, y_test_pred_prob)\n",
    "plot_roc_curve(fpr_test, tpr_test, 'Test Set ROC Curve Logit')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For logistic regression, we have achieved the following results:\n",
    "AUC for test sample of 0.77 (test sample from train dataset) accuracy of 0.77 and balanced accuracy of 0.7 as we decided to use G-mean as a metric.\n",
    "F1 score, as well as precision and recall are provided in table above. \n",
    "\n",
    "Data set provided by ING will be used in last section of the analysis to compare the results of logistic regression with ensemble of (XGBoost and Neural Network models)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 \n",
    "Traditional models like logistic regression, while useful, often struggle to capture the full complexity and nuances present in modern credit datasets. This is where machine learning models like XGBoost and neural networks can shine. In this section, we will train and evaluate both a XGBoost model and a neural network model on the same dataset and compare their performance to the logistic regression model. As our final model we will use an ensemble of the three models to get the best of all worlds. \n",
    "\n",
    "#### XGBoost \n",
    "XGBoost, short for eXtreme Gradient Boosting, is a highly efficient and scalable implementation of gradient boosting. It operates by constructing an ensemble of decision trees in a sequential manner, with each tree being built to address and correct the mistakes of its predecessors. To enhance the performance of an XGBoost model, hyperparameter tuning is crucial. This is where Optuna comes into play. Optuna is an open-source optimization framework designed to automate the process of finding the best hyperparameters for machine learning models. It efficiently searches the hyperparameter space using Bayesian optimization, gradient-based optimization, or evolutionary algorithms, significantly reducing the time and effort required for manual tuning. In addition to model and hyperparameter optimization, handling imbalanced datasets is another critical aspect of building effective machine learning models. This is particularly important in classification tasks where the distribution of classes is skewed. To address this, Synthetic Minority Over-sampling Technique (SMOTE) is employed. This oversampling technique helps to improve model performance, especially for minority classes, by providing a more balanced dataset for training. It is worth noting that data standardization is not a prerequisite as decision trees is the building blocks of XGBoost. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# XGB - same dataset as before\n",
    "X_train_XGB = X_train\n",
    "X_val_XGB = X_val\n",
    "X_test_XGB = X_test\n",
    "y_train_XGB = y_train\n",
    "y_val_XGB = y_val\n",
    "y_test_XGB = y_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Apply SMOTE to oversample the minority class in the training dataset\n",
    "smote = SMOTE(random_state=1234)\n",
    "X_train_XGB_resampled, y_train_XGB_resampled = smote.fit_resample(X_train_XGB, y_train_XGB)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'auc',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 25, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.05),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 100),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 20, 1000),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-2, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-2, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**{k: v for k, v in param.items() if k != 'early_stopping_rounds'})\n",
    "    model.set_params(**{'early_stopping_rounds': 100})\n",
    "    model.fit(X_train_XGB_resampled, y_train_XGB_resampled, eval_set=[(X_val_XGB, y_val_XGB)], verbose=False)\n",
    "    \n",
    "    preds_proba = model.predict_proba(X_val_XGB)[:, 1]\n",
    "    auc_score = roc_auc_score(y_val_XGB, preds_proba)\n",
    "    \n",
    "    return auc_score\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('Best trial:', study.best_trial.params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "model_XGB = XGBClassifier(**study.best_trial.params)\n",
    "model_XGB.fit(X_train_XGB_resampled, y_train_XGB_resampled, eval_set=[(X_val, y_val)], verbose=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "predictions = model_XGB.predict(X_test_XGB)\n",
    "print(f\"Balanced accuracy: {balanced_accuracy_score(y_test_XGB, predictions)}\")\n",
    "print(f\"Accuracy         : {accuracy_score(y_test_XGB, predictions)}\")\n",
    "\n",
    "# Evaluate the model using the simple evaluation function\n",
    "print(\"Train set XGB:\")\n",
    "evaluate_predictions(y_train_XGB, model_XGB.predict(X_train_XGB))\n",
    "print(\"Validation set XGB:\")\n",
    "evaluate_predictions(y_val_XGB, model_XGB.predict(X_val_XGB))\n",
    "print(\"Test set XGB:\")\n",
    "evaluate_predictions(y_test_XGB, predictions)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_train_pred_prob =  model_XGB.predict_proba(X_train_XGB)[:, 1]\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train.astype(int).values, y_train_pred_prob)\n",
    "plot_roc_curve(fpr_train, tpr_train, 'Training Set ROC Curve XGB')\n",
    "\n",
    "y_val_pred_prob = model_XGB.predict_proba(X_val_XGB)[:, 1]\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val.astype(int).values, y_val_pred_prob)\n",
    "plot_roc_curve(fpr_val, tpr_val, 'Validation Set ROC Curve XGB')\n",
    "\n",
    "y_test_pred_prob = model_XGB.predict_proba(X_test_XGB)[:, 1]\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test.astype(int).values, y_test_pred_prob)\n",
    "plot_roc_curve(fpr_test, tpr_test, 'Test Set ROC Curve XGB')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural network \n",
    "\n",
    "In this section, we will train a neural network model using PyTorch. We will use the same dataset as before and use the same train-test split. We will use 2 layers 256, 128 and output layer. We also added dropout_rate of 0.15, learning rate of 0.002 that is decaying multiplicatively by 0.5 every 10 epochs and weight decay which is L2 regularization of 0.000001. We will use the BCEWithLogitsLoss as the loss function and Adam as the optimizer. We will also use the class weights to handle the class imbalance. We will train the model for 70 epochs and evaluate it on the validation set. \n",
    "\n",
    "Which is not shown in the code for simplicity is random search on much wider range of hyperparameters, that was computed to minimize loss on validation set. We provided only the final hyperparameters below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Neural network\n",
    "X_train_NN = X_train\n",
    "X_val_NN = X_val\n",
    "X_test_NN = X_test\n",
    "y_train_NN = y_train\n",
    "y_val_NN = y_val\n",
    "y_test_NN = y_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_NN = scaler.fit_transform(X_train_NN)\n",
    "X_val_NN = scaler.transform(X_val_NN)\n",
    "X_test_NN = scaler.transform(X_test_NN)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_NN, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_NN.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val_NN, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_NN.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_NN, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_NN.to_numpy(), dtype=torch.float32).unsqueeze(1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BinaryClassificationNN(nn.Module):\n",
    "    def __init__(self, input_size, dropout_rate=0.15):\n",
    "        super(BinaryClassificationNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.relu1(self.layer1(x)))\n",
    "        x = self.dropout2(self.relu2(self.layer2(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "classes = np.unique(y_train_tensor.numpy())\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_tensor.numpy().flatten())\n",
    "class_weights_tensor = torch.tensor(class_weights[1], dtype=torch.float32)\n",
    "\n",
    "input_size = X_train_NN.shape[1]\n",
    "model_NN = BinaryClassificationNN(input_size)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model_NN.parameters(), lr=0.002, weight_decay=0.000001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "num_epochs = 70\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model_NN(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "with torch.no_grad():\n",
    "    val_outputs = model_NN(X_val_tensor)\n",
    "    val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    val_predictions = torch.sigmoid(val_outputs)\n",
    "    predicted_classes = (val_predictions >= 0.5).float()\n",
    "    # Calculate AUC\n",
    "    auc_score = roc_auc_score(y_val_tensor.numpy(), val_predictions.numpy())\n",
    "    print(f'Validation Loss: {val_loss.item():.4f}, AUC: {auc_score:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def logits_to_binary_predictions(logits, threshold=0.5):\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    return (probabilities >= threshold).int()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_and_plot(model, X_tensor, y_true_tensor, dataset_name='Dataset'):\n",
    "    # Get the logits from your model\n",
    "    logits = model(X_tensor)\n",
    "    \n",
    "    # Convert logits to binary predictions and probabilities\n",
    "    predictions = logits_to_binary_predictions(logits, threshold=0.5).numpy().squeeze()\n",
    "    probabilities = torch.sigmoid(logits).numpy().squeeze()\n",
    "    \n",
    "    # True labels\n",
    "    y_true = y_true_tensor.numpy()\n",
    "    \n",
    "    # Evaluate predictions using your existing function\n",
    "    print(f\"{dataset_name} evaluation:\")\n",
    "    evaluate_predictions(y_true, predictions)\n",
    "    \n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_true, probabilities)\n",
    "    plot_roc_curve(fpr, tpr, title=f'{dataset_name} ROC Curve')\n",
    "\n",
    "# Now, call this function for each of your datasets: training, validation, and test sets.\n",
    "evaluate_and_plot(model_NN, X_train_tensor, y_train_tensor, 'Training Set NN')\n",
    "evaluate_and_plot(model_NN, X_val_tensor, y_val_tensor, 'Validation Set NN')\n",
    "evaluate_and_plot(model_NN, X_test_tensor, y_test_tensor, 'Test Set NN')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_probabilities = torch.sigmoid(model_NN(X_test_tensor)).detach().numpy().squeeze()\n",
    "print(test_probabilities.min(), test_probabilities.max())\n",
    "test_probabilities\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ENSEMBLE MODEL\n",
    "\n",
    "For our final proposition of the model we decided to use an ensemble of 2 models as average of probabilities from XGboost model and Neural Network. We calculated best cut off based on validation set and G-means value as mentioned before we don't have information about costs of wrong decisions and missed potential profits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ENSEMBLE MODEL \n",
    "# Predictions from each model\n",
    "y_test_logit_prob = model_Logit.predict(X_test_logit)\n",
    "y_test_XGB_prob = model_XGB.predict_proba(X_test_XGB)[:, 1]\n",
    "y_test_NN_prob = torch.sigmoid(model_NN(X_test_tensor)).detach().numpy().squeeze()\n",
    "\n",
    "ensemble_prob = (y_test_XGB_prob + y_test_NN_prob)/2\n",
    "print(ensemble_prob)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for the logit model\n",
    "y_test_pred_prob = model_Logit.predict(X_test_logit)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test.astype(int).values, y_test_pred_prob)\n",
    "plot_roc_curve(fpr_test, tpr_test, 'Test Set ROC Curve logit')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fpr_test, tpr_test, _ = roc_curve(y_test.astype(int).values, ensemble_prob)\n",
    "plot_roc_curve(fpr_test, tpr_test, 'Test Set ROC Curve Ensemble')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for the logistic regression model\n",
    "fpr_logit, tpr_logit, thresholds_logit = roc_curve(y_test.astype(int).values, y_test_pred_prob)\n",
    "roc_auc_logit = auc(fpr_logit, tpr_logit)\n",
    "\n",
    "# Compute ROC curve and ROC area for the ensemble model\n",
    "fpr_ensemble, tpr_ensemble, thresholds_ensemble = roc_curve(y_test.astype(int).values, ensemble_prob)\n",
    "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
    "\n",
    "# Plotting both ROC curves on the same plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_logit, tpr_logit, color='blue', label='Logit ROC curve (area = %0.2f)' % roc_auc_logit)\n",
    "plt.plot(fpr_ensemble, tpr_ensemble, color='darkorange', label='Ensemble ROC curve (area = %0.2f)' % roc_auc_ensemble)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculate best cut off based on validation set\n",
    "y_val_XGB_prob = model_XGB.predict_proba(X_val_XGB)[:, 1]\n",
    "y_val_NN_prob = torch.sigmoid(model_NN(X_val_tensor)).detach().numpy().squeeze()\n",
    "ensemble_prob_val = (y_val_XGB_prob + y_val_NN_prob)/2\n",
    "fpr, tpr, thresholds = roc_curve(y_val, ensemble_prob_val)\n",
    "gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "ix = np.argmax(gmeans)\n",
    "optimal_cutoff_ensemble = thresholds[ix]\n",
    "\n",
    "# Evaluate the ensemble model using the simple evaluation function\n",
    "ensemble_predictions = (ensemble_prob >= optimal_cutoff_ensemble).astype(int)\n",
    "print(\"Test set Ensemble:\")\n",
    "evaluate_predictions(y_test, ensemble_predictions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets compare results on our test sample seperated from train dataset. For logistic regression we have achieved AUC of 0.77 while for ensemble model we have achieved AUC of 0.78. We can see that ensemble model is slightly better than logistic regression model in this case. For accuracy we have 0.76 for logistic regression and 0.74 for ensemble model. Balanced accuracy is 0.701 for logistic regression and 0.703 for ensemble model. Rest statistics also point out that both models have similar performance. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "now let's analyze the data set provided by ING, in real life we wouldn't have correct predictions for this data set, but we can use it to compare the results of our models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## data set provided by ING \n",
    "# same transformations as before\n",
    "test = pd.read_csv('https://files.challengerocket.com/files/lions-den-ing-2024/testing_sample.csv')\n",
    "df_test = test[test['Application_status'] == 'Approved'].copy()\n",
    "\n",
    "d1_test = pd.to_datetime(df_test['application_date'].copy(), format='%d%b%Y %H:%M:%S', errors='coerce')\n",
    "d2_test = pd.to_datetime(df_test['Var13'], format='%d%b%Y', errors='coerce')\n",
    "df_test[days_from_employment] = (d1_test - d2_test).dt.days\n",
    "\n",
    "df_test['Var17'].fillna(df_test['Var17'].median(), inplace=True)\n",
    "\n",
    "for var in fill_with_zero:\n",
    "    df_test[var].fillna(0, inplace=True)\n",
    "\n",
    "for var in add_other_category:\n",
    "    df_test[var].fillna('other', inplace=True)\n",
    "\n",
    "df_test.drop(columns=not_features + other_to_drop + columns_to_drop, inplace=True)\n",
    "\n",
    "need_dummies = set(categorical_features) - set(columns_to_drop)\n",
    "\n",
    "for feature in need_dummies:\n",
    "    one_hot = pd.get_dummies(df_test[feature], prefix=feature, drop_first=True).astype(int)\n",
    "    df_test = df_test.drop(feature, axis=1)\n",
    "    df_test = df_test.join(one_hot)\n",
    "\n",
    "# Add missing categories to get the same shape \n",
    "right_order = X.columns\n",
    "df_test = df_test.rename(columns={'Var3_3.0':'Var3_3', 'Var3_2.0':'Var3_2'})\n",
    "df_test['Var3_Direct'] = 0\n",
    "df_test['Var3_Online'] = 0\n",
    "\n",
    "X_test_2 = df_test.drop(columns=['target'])[right_order]\n",
    "Y_test_2 = df_test['target']\n",
    "\n",
    "X_test_logit_2 = scaler.fit_transform(X_test_2)\n",
    "\n",
    "test_predictions_NN = torch.sigmoid(model_NN(torch.tensor(X_test_logit_2, dtype=torch.float32))).detach().numpy().squeeze()\n",
    "test_predictions_XGB = model_XGB.predict_proba(X_test_2)[:, 1]\n",
    "ensemble_prob_test = (test_predictions_NN + test_predictions_XGB)/2\n",
    "ensemble_predictions_test = (ensemble_prob_test >= optimal_cutoff_ensemble).astype(int)\n",
    "print(\"Test set ensemble ING:\")\n",
    "evaluate_predictions(ensemble_predictions_test, Y_test_2)\n",
    "\n",
    "# roc curve\n",
    "fpr_test, tpr_test, _ = roc_curve(Y_test_2.astype(int).values, ensemble_prob_test)\n",
    "plot_roc_curve(fpr_test, tpr_test, 'ING Test Set ROC Curve Ensemble')\n",
    "\n",
    "# compare with logit model\n",
    "test_probabilities = model_Logit.predict(X_test_logit_2)\n",
    "logit_predictions_test = (test_probabilities >= optimal_cutoff_Logit).astype(int)\n",
    "\n",
    "print(\"Test set Logit ING:\")\n",
    "evaluate_predictions(logit_predictions_test, Y_test_2)\n",
    "fpr_test, tpr_test, _ = roc_curve(Y_test_2.astype(int).values, test_probabilities)\n",
    "plot_roc_curve(fpr_test, tpr_test, 'ING Test Set ROC Curve Logit')\n",
    "\n",
    "# Compute ROC curve and ROC area for the logistic regression model\n",
    "fpr_logit, tpr_logit, thresholds_logit = roc_curve(Y_test_2.astype(int).values, test_probabilities)\n",
    "roc_auc_logit = auc(fpr_logit, tpr_logit)\n",
    "\n",
    "# Compute ROC curve and ROC area for the ensemble model\n",
    "fpr_ensemble, tpr_ensemble, thresholds_ensemble = roc_curve(Y_test_2.astype(int).values, ensemble_prob_test)\n",
    "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
    "\n",
    "# Plotting both ROC curves on the same plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_logit, tpr_logit, color='blue', label='Logit ROC curve (area = %0.2f)' % roc_auc_logit)\n",
    "plt.plot(fpr_ensemble, tpr_ensemble, color='darkorange', label='Ensemble ROC curve (area = %0.2f)' % roc_auc_ensemble)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final summary for 1.2 task\n",
    "\n",
    "Our main statistic to maximise was AUC. As accuracy and balanced accuracy can be changed by setting a different cut-off point, for both models G-means statistic was used. AUC on ING test sample for logistic regression was 0.71 while ensemble model achieved 0.74 which is a significant improvement. For Accuracy Logit has 0.71 while ensemble 0.75 and for balanced accuracy Logit has 0.52 while ensemble 0.53. As we can see Ensemble model outperformed logistic regression model in all metrics (as well for F1, precision and recall). However, it is worth to point out that with more complicated models like ensemble of different so called \"black box\" models, it is harder to interpret the results and explain the model to the client. One of the biggest challenges with ensemble models is their lack of interpretability compared to simpler models like logistic regression. In the banking sector, where regulatory compliance and the ability to explain decisions to customers are paramount, the \"black box\" nature of ensemble models can be a significant drawback. We would need to perform very hard computations with methods like SHAP or LIME to explain the results. Which we haven't done in this analysis due to the time constraints. In real life this would be hard to implement in banking sector as validation of the model would be hard to perform. \n",
    "\n",
    "Jedrzej Maskiewicz, Adam Ginna, Wiktor Morawski, Grzegorz Szot\n",
    "\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
